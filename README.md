In the paper “Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training,” it was shown that models can behave deceptively and change their behavior in different contexts. For example, they may write good and secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. In this project, we are exploring the mechanistic effects of training a GPT-2-sized model on a poisoned dataset and collecting statistics on how much poisoning affects the model.

Produced in collaboration with https://github.com/Tookser/
